{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6feb43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from AutoEncoderCNN import AE_CNN\n",
    "from GridSearch import GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b710ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "torch.set_default_device(device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d0c6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        path = self.imgs[index][0]\n",
    "        \n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60e1d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/groups/francescavitali/eb2/NewsubSubImages4/H&E' # /groups/francescavitali/eb2/subImages_slide299/H&E\n",
    "\n",
    "SPLIT = [55767, 6971, 6971]\n",
    "\n",
    "tensor_transform = transforms.ToTensor()\n",
    "\n",
    "dataset = ImageFolderWithPaths(PATH, transform = tensor_transform)\n",
    "\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset,\n",
    "                                                           SPLIT,# 80%, 10%, 10%\n",
    "                                                           generator=torch.Generator(device=device))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_set,\n",
    "                                            batch_size = 1,\n",
    "                                            shuffle = True,\n",
    "                                            generator=torch.Generator(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd60b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AE_CNN().to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('./models/model_gs.pth')) # loading best model state\n",
    "\n",
    "# setting the encoder\n",
    "encoder = model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d82e6a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(8, 8), stride=(2, 2), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(64, 32, kernel_size=(8, 8), stride=(2, 2), padding=(1, 1))\n",
       "  (3): ReLU()\n",
       "  (4): Conv2d(32, 26, kernel_size=(8, 8), stride=(2, 2), padding=(1, 1))\n",
       "  (5): ReLU()\n",
       "  (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  (7): Linear(in_features=28314, out_features=28314, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0db4d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28314, 16384),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16384, 4096),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(4096, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 1),\n",
    "        )\n",
    "        self._sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    \n",
    "    def forward(self,  x):\n",
    "        output = self._feed_forward(x) \n",
    "        return self._sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ecc0093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNet().to(device)\n",
    "nn.load_state_dict(torch.load('./ClassifierModels/class_model_gs.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8f9d9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.24%\n"
     ]
    }
   ],
   "source": [
    "ans = []\n",
    "total_samples = 0\n",
    "total_correct = 0\n",
    "nn.to(device)\n",
    "nn.eval()\n",
    "for (image, label, fname) in test_loader:\n",
    "    nn.eval()\n",
    "    image = image.to(device)\n",
    "    label = label.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # feeding through nn\n",
    "        encoded = encoder(image)\n",
    "        not_rounded = nn._feed_forward(encoded)\n",
    "        outputs = nn._sigmoid(not_rounded)\n",
    "        \n",
    "        # results based on sigmoid\n",
    "        if outputs < 0.5: \n",
    "            outputs = 0\n",
    "        else:\n",
    "            outputs = 1\n",
    "            \n",
    "        # for calculating percentage and visualizing\n",
    "        total_correct += (outputs == label).item()\n",
    "        ans.append((label.item(), outputs, fname))\n",
    "\n",
    "        \n",
    "print(f'Accuracy: {total_correct/len(test_loader)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b2dff00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 0,\n",
       " ('/groups/francescavitali/eb2/NewsubSubImages4/H&E/S/091/02-091_04/013_015.png',))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "928b7442",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {} # d[patient #] = [goal, # of 1's guessed, total images] \n",
    "for i in range(len(ans)):\n",
    "    patient = ans[i][2][0].split('/')[7]\n",
    "    if patient in d:\n",
    "        d[patient][1] += ans[i][1]\n",
    "        d[patient][2] += 1\n",
    "    else:\n",
    "        d[patient] = [ans[i][0], ans[i][1], 1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "349ea26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001: [0, 6, 62]\n",
      "002: [0, 3, 111]\n",
      "004: [0, 2, 91]\n",
      "005: [0, 1, 83]\n",
      "006: [0, 3, 38]\n",
      "007: [0, 4, 29]\n",
      "008: [0, 4, 130]\n",
      "009: [0, 4, 105]\n",
      "010: [0, 1, 49]\n",
      "011: [0, 1, 87]\n",
      "012: [0, 3, 87]\n",
      "013: [0, 2, 72]\n",
      "014: [0, 1, 62]\n",
      "015: [0, 4, 60]\n",
      "016: [0, 6, 110]\n",
      "017: [0, 2, 142]\n",
      "018: [0, 1, 62]\n",
      "019: [0, 2, 155]\n",
      "020: [0, 13, 120]\n",
      "021: [0, 0, 31]\n",
      "022: [0, 6, 99]\n",
      "023: [0, 3, 115]\n",
      "024: [0, 2, 63]\n",
      "025: [0, 7, 115]\n",
      "026: [0, 2, 12]\n",
      "027: [0, 6, 107]\n",
      "028: [0, 4, 89]\n",
      "029: [0, 0, 5]\n",
      "030: [0, 6, 137]\n",
      "031: [0, 5, 98]\n",
      "032: [0, 1, 15]\n",
      "033: [0, 4, 109]\n",
      "034: [0, 6, 92]\n",
      "035: [0, 2, 53]\n",
      "036: [0, 3, 134]\n",
      "037: [0, 1, 91]\n",
      "038: [0, 1, 121]\n",
      "039: [0, 0, 43]\n",
      "040: [0, 8, 168]\n",
      "041: [0, 2, 65]\n",
      "042: [0, 12, 114]\n",
      "044: [0, 0, 105]\n",
      "045: [0, 0, 14]\n",
      "046: [0, 3, 110]\n",
      "047: [0, 4, 33]\n",
      "048: [0, 3, 24]\n",
      "049: [0, 4, 46]\n",
      "050: [0, 4, 130]\n",
      "052: [0, 3, 93]\n",
      "053: [0, 3, 70]\n",
      "054: [0, 3, 26]\n",
      "055: [0, 2, 75]\n",
      "057: [0, 1, 27]\n",
      "058: [1, 8, 82]\n",
      "059: [1, 2, 88]\n",
      "060: [1, 11, 88]\n",
      "063: [1, 8, 86]\n",
      "064: [1, 5, 71]\n",
      "065: [1, 17, 151]\n",
      "066: [1, 4, 29]\n",
      "067: [1, 0, 27]\n",
      "068: [1, 3, 92]\n",
      "069: [1, 2, 138]\n",
      "070: [1, 12, 96]\n",
      "071: [1, 3, 117]\n",
      "072: [1, 4, 46]\n",
      "073: [1, 4, 99]\n",
      "074: [1, 0, 49]\n",
      "075: [1, 8, 87]\n",
      "076: [1, 2, 47]\n",
      "077: [1, 1, 40]\n",
      "078: [1, 1, 109]\n",
      "079: [1, 5, 92]\n",
      "080: [1, 8, 74]\n",
      "081: [1, 8, 83]\n",
      "082: [1, 7, 77]\n",
      "083: [1, 4, 68]\n",
      "084: [1, 0, 182]\n",
      "086: [1, 3, 109]\n",
      "087: [1, 12, 72]\n",
      "088: [1, 4, 60]\n",
      "090: [1, 11, 188]\n",
      "091: [1, 2, 89]\n",
      "092: [1, 0, 51]\n",
      "min(total_images)=5 | max(total_images)=188\n"
     ]
    }
   ],
   "source": [
    "total_images = []\n",
    "for key in sorted(list(d.keys())):\n",
    "    total_images.append(d[key][2])\n",
    "    print(f'{key}: {d[key]}')\n",
    "print(f'{min(total_images)=} | {max(total_images)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "235b924d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.98809523809524\n"
     ]
    }
   ],
   "source": [
    "print(sum(total_images)/len(total_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bddfae62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4., 10.,  8., 15., 18., 13.,  7.,  5.,  2.,  2.]),\n",
       " array([  5. ,  23.3,  41.6,  59.9,  78.2,  96.5, 114.8, 133.1, 151.4,\n",
       "        169.7, 188. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(total_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c198d3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with thresh=1: 46.43%\n",
      "Accuracy with thresh=2: 52.38%\n",
      "Accuracy with thresh=3: 60.71%\n",
      "Accuracy with thresh=4: 65.48%\n",
      "Accuracy with thresh=5: 64.29%\n",
      "Accuracy with thresh=6: 71.43%\n",
      "Accuracy with thresh=7: 71.43%\n",
      "Accuracy with thresh=8: 66.67%\n",
      "Accuracy with thresh=9: 66.67%\n",
      "Accuracy with thresh=10: 66.67%\n",
      "Accuracy with thresh=11: 64.29%\n",
      "Accuracy with thresh=12: 63.10%\n",
      "Accuracy with thresh=13: 64.29%\n",
      "Accuracy with thresh=14: 64.29%\n",
      "Accuracy with thresh=15: 64.29%\n",
      "Accuracy with thresh=16: 64.29%\n",
      "Accuracy with thresh=17: 63.10%\n"
     ]
    }
   ],
   "source": [
    "max_thresh = 18\n",
    "for thresh in range(1,max_thresh):\n",
    "    correct = 0\n",
    "    for patient, val in d.items():\n",
    "        guess = 0\n",
    "        if val[1] > thresh:\n",
    "            guess = 1\n",
    "        if guess == val[0]:\n",
    "            correct += 1\n",
    "    print(f'Accuracy with {thresh=}: {correct/len(d.keys())*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc5a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
