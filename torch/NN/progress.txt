AE=AE_CNN(
  (softmax): Softmax(dim=None)
  (encoder): Sequential(
    (0): Conv2d(3, 64, kernel_size=(8, 8), stride=(2, 2), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(64, 32, kernel_size=(8, 8), stride=(2, 2), padding=(1, 1))
    (3): ReLU()
    (4): Conv2d(32, 20, kernel_size=(8, 8), stride=(2, 2), padding=(1, 1))
    (5): ReLU()
    (6): Flatten(start_dim=1, end_dim=-1)
    (7): Linear(in_features=21780, out_features=21780, bias=True)
    (8): ReLU()
    (9): Linear(in_features=21780, out_features=4096, bias=True)
  )
  (decoder): Sequential(
    (0): Linear(in_features=4096, out_features=4096, bias=True)
    (1): ReLU()
    (2): Linear(in_features=4096, out_features=21780, bias=True)
    (3): ReLU()
    (4): Unflatten(dim=1, unflattened_size=(20, 33, 33))
    (5): ConvTranspose2d(20, 32, kernel_size=(8, 8), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
    (6): ReLU()
    (7): ConvTranspose2d(32, 64, kernel_size=(8, 8), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
    (8): ReLU()
    (9): ConvTranspose2d(64, 3, kernel_size=(8, 8), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
    (10): Sigmoid()
  )
)

Training:

Epoch: 1 | Loss: 0.0170


Epoch: 2 | Loss: 0.0132


Epoch: 3 | Loss: 0.0159


Epoch: 4 | Loss: 0.0126


Epoch: 5 | Loss: 0.0132


Epoch: 6 | Loss: 0.0131


Epoch: 7 | Loss: 0.0132


Epoch: 8 | Loss: 0.0153


Epoch: 9 | Loss: 0.0142


Epoch: 10 | Loss: 0.0134


Epoch: 11 | Loss: 0.0151


Epoch: 12 | Loss: 0.0139


Epoch: 13 | Loss: 0.0133


Epoch: 14 | Loss: 0.0143


Epoch: 15 | Loss: 0.0136


Epoch: 16 | Loss: 0.0146


Epoch: 17 | Loss: 0.0119


Epoch: 18 | Loss: 0.0147


Epoch: 19 | Loss: 0.0129


Epoch: 20 | Loss: 0.0120


Epoch: 21 | Loss: 0.0132


Epoch: 22 | Loss: 0.0129


Epoch: 23 | Loss: 0.0129


Epoch: 24 | Loss: 0.0133


Epoch: 25 | Loss: 0.0146


Epoch: 26 | Loss: 0.0144


Epoch: 27 | Loss: 0.0137


Epoch: 28 | Loss: 0.0130


Epoch: 29 | Loss: 0.0130


Epoch: 30 | Loss: 0.0137


Epoch: 31 | Loss: 0.0119


Epoch: 32 | Loss: 0.0129


Epoch: 33 | Loss: 0.0132


Epoch: 34 | Loss: 0.0128


Epoch: 35 | Loss: 0.0156


Epoch: 36 | Loss: 0.0135


Epoch: 37 | Loss: 0.0141


Epoch: 38 | Loss: 0.0136


Epoch: 39 | Loss: 0.0122


Epoch: 40 | Loss: 0.0134


Epoch: 41 | Loss: 0.0138


Epoch: 42 | Loss: 0.0128


Epoch: 43 | Loss: 0.0135


Epoch: 44 | Loss: 0.0140


Epoch: 45 | Loss: 0.0133


Epoch: 46 | Loss: 0.0138


Epoch: 47 | Loss: 0.0145


Epoch: 48 | Loss: 0.0135


Epoch: 49 | Loss: 0.0138


Epoch: 50 | Loss: 0.0151


Epoch: 51 | Loss: 0.0135


Epoch: 52 | Loss: 0.0137


Epoch: 53 | Loss: 0.0133


Epoch: 54 | Loss: 0.0134


Epoch: 55 | Loss: 0.0156


Epoch: 56 | Loss: 0.0133


Epoch: 57 | Loss: 0.0142


Epoch: 58 | Loss: 0.0136


Epoch: 59 | Loss: 0.0129


Epoch: 60 | Loss: 0.0143


Epoch: 61 | Loss: 0.0130


Epoch: 62 | Loss: 0.0136


Epoch: 63 | Loss: 0.0136


Epoch: 64 | Loss: 0.0138


Epoch: 65 | Loss: 0.0138


Epoch: 66 | Loss: 0.0140


Epoch: 67 | Loss: 0.0137


Epoch: 68 | Loss: 0.0148


Epoch: 69 | Loss: 0.0136


Epoch: 70 | Loss: 0.0124


Epoch: 71 | Loss: 0.0149


Epoch: 72 | Loss: 0.0136


Epoch: 73 | Loss: 0.0150


Epoch: 74 | Loss: 0.0135


Epoch: 75 | Loss: 0.0140

