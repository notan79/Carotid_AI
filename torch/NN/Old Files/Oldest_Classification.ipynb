{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a013e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from AutoEncoderCNN import AE_CNN\n",
    "from GridSearch import GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25a2eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a89fe42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb97273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/groups/francescavitali/eb2/subImages_slide299/H&E' # has 506 images\n",
    "BATCH_SIZE = 4 # make sure divisble by 404: 1, 2, 4, 101, 202, 404\n",
    "\n",
    "tensor_transform = transforms.ToTensor()\n",
    "\n",
    "dataset = datasets.ImageFolder(PATH, \n",
    "                               transform = tensor_transform) #loads the images\n",
    "\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset,\n",
    "                                                           [404,51,51],# 80%, 10%, 10%\n",
    "                                                           generator=torch.Generator(device=device))\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset = train_set,\n",
    "                                            batch_size = BATCH_SIZE,\n",
    "                                            shuffle = True,\n",
    "                                            generator=torch.Generator(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ecbc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16cf6002",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for AE_CNN:\n\tMissing key(s) in state_dict: \"encoder.4.weight\", \"encoder.4.bias\", \"encoder.7.weight\", \"encoder.7.bias\", \"decoder.1.weight\", \"decoder.1.bias\", \"decoder.3.weight\", \"decoder.3.bias\", \"decoder.5.weight\", \"decoder.5.bias\". \n\tUnexpected key(s) in state_dict: \"decoder.0.weight\", \"decoder.0.bias\", \"decoder.2.weight\", \"decoder.2.bias\". \n\tsize mismatch for encoder.0.weight: copying a param with shape torch.Size([64, 3, 16, 16]) from checkpoint, the shape in current model is torch.Size([64, 3, 8, 8]).\n\tsize mismatch for encoder.2.weight: copying a param with shape torch.Size([128, 64, 16, 16]) from checkpoint, the shape in current model is torch.Size([32, 64, 8, 8]).\n\tsize mismatch for encoder.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m AE_CNN(\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m64\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#model.load_state_dict(torch.load('./models/model_gs.pth')) # loading best model state\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./models/Copy Models/model_gs_3-28-2024.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# setting the encoder and decoder for visualization\u001b[39;00m\n\u001b[1;32m      7\u001b[0m encoder \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencoder\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for AE_CNN:\n\tMissing key(s) in state_dict: \"encoder.4.weight\", \"encoder.4.bias\", \"encoder.7.weight\", \"encoder.7.bias\", \"decoder.1.weight\", \"decoder.1.bias\", \"decoder.3.weight\", \"decoder.3.bias\", \"decoder.5.weight\", \"decoder.5.bias\". \n\tUnexpected key(s) in state_dict: \"decoder.0.weight\", \"decoder.0.bias\", \"decoder.2.weight\", \"decoder.2.bias\". \n\tsize mismatch for encoder.0.weight: copying a param with shape torch.Size([64, 3, 16, 16]) from checkpoint, the shape in current model is torch.Size([64, 3, 8, 8]).\n\tsize mismatch for encoder.2.weight: copying a param with shape torch.Size([128, 64, 16, 16]) from checkpoint, the shape in current model is torch.Size([32, 64, 8, 8]).\n\tsize mismatch for encoder.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32])."
     ]
    }
   ],
   "source": [
    "model = AE_CNN(32,64).to(device)\n",
    "\n",
    "#model.load_state_dict(torch.load('./models/model_gs.pth')) # loading best model state\n",
    "model.load_state_dict(torch.load('./models/Copy Models/model_gs_3-28-2024.pth'))\n",
    "\n",
    "# setting the encoder and decoder for visualization\n",
    "encoder = model.encoder\n",
    "decoder = model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5fe0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "with torch.no_grad():\n",
    "    for(img, goal) in loader: # goal will be a tensor of len == batch_size\n",
    "        if count == 1:\n",
    "            break\n",
    "        img = img.to(device)\n",
    "        print(f'Img Shape: {img.shape}')\n",
    "        encoded_img = encoder(img)\n",
    "        print(f'Encoded Shape: {encoded_img.shape}')\n",
    "        flattened = encoded_img.flatten(start_dim = 1)\n",
    "        print(f'Flattened: {flattened.shape}')\n",
    "        print(f'Goal: {goal.unsqueeze(1)}')\n",
    "        print()\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a36303",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self._feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(56448, 8192),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(32768, 16384),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(16384, 8192),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(8192, 2048),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(2048, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 1),\n",
    "        )\n",
    "        self._sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def encoded_without_training(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.encoder.eval()\n",
    "            encoded = self.encoder(x)\n",
    "            flattened = encoded.flatten(start_dim = 1)\n",
    "            return flattened\n",
    "    \n",
    "    def forward(self,  x):\n",
    "        self.train()\n",
    "        encoded = self.encoder(x) # this will update the encoder weights\n",
    "        flattened = encoded.flatten(start_dim = 1)\n",
    "#         flattened = self.encoded_without_training(x)\n",
    "        output = self._feed_forward(flattened)\n",
    "        return self._sigmoid(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20ca5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNet(encoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "849985bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00003\n",
    "weight_decay = 1e-5\n",
    "EPOCHS = 50\n",
    "\n",
    "verbose = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06eaa0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(dataset = val_set,\n",
    "                                            batch_size = 1,\n",
    "                                            shuffle = True,\n",
    "                                            generator=torch.Generator(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fb6d795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [====================================================================================================] 100.0% | Loss: 0.523341715335846\n",
      "Epoch: 1 | Loss: 0.5233 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 2 [====================================================================================================] 100.0% | Loss: 0.6720530986785889\n",
      "Epoch: 2 | Loss: 0.6721 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 3 [====================================================================================================] 100.0% | Loss: 0.5455887317657471\n",
      "Epoch: 3 | Loss: 0.5456 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 4 [====================================================================================================] 100.0% | Loss: 0.5480521917343143\n",
      "Epoch: 4 | Loss: 0.5481 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 5 [====================================================================================================] 100.0% | Loss: 0.545356035232544\n",
      "Epoch: 5 | Loss: 0.5454 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 6 [====================================================================================================] 100.0% | Loss: 0.5850424766540527\n",
      "Epoch: 6 | Loss: 0.5850 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 7 [====================================================================================================] 100.0% | Loss: 0.5486681461334229\n",
      "Epoch: 7 | Loss: 0.5487 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 8 [====================================================================================================] 100.0% | Loss: 0.5552769303321838\n",
      "Epoch: 8 | Loss: 0.5553 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 9 [====================================================================================================] 100.0% | Loss: 0.5825492143630981\n",
      "Epoch: 9 | Loss: 0.5825 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 10 [====================================================================================================] 100.0% | Loss: 0.5128298997879028\n",
      "Epoch: 10 | Loss: 0.5128 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 11 [====================================================================================================] 100.0% | Loss: 0.5708677172660828\n",
      "Epoch: 11 | Loss: 0.5709 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 12 [====================================================================================================] 100.0% | Loss: 0.5677100419998169\n",
      "Epoch: 12 | Loss: 0.5677 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 13 [====================================================================================================] 100.0% | Loss: 0.5794917941093445\n",
      "Epoch: 13 | Loss: 0.5795 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 14 [====================================================================================================] 100.0% | Loss: 0.5598975419998169\n",
      "Epoch: 14 | Loss: 0.5599 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 15 [====================================================================================================] 100.0% | Loss: 0.5533614754676819\n",
      "Epoch: 15 | Loss: 0.5534 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 16 [====================================================================================================] 100.0% | Loss: 0.5597178936004639\n",
      "Epoch: 16 | Loss: 0.5597 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 17 [====================================================================================================] 100.0% | Loss: 0.5627297163009644\n",
      "Epoch: 17 | Loss: 0.5627 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 18 [====================================================================================================] 100.0% | Loss: 0.5675225257873535\n",
      "Epoch: 18 | Loss: 0.5675 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 19 [====================================================================================================] 100.0% | Loss: 0.5403748750686646\n",
      "Epoch: 19 | Loss: 0.5404 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 20 [====================================================================================================] 100.0% | Loss: 0.5635651350021362\n",
      "Epoch: 20 | Loss: 0.5636 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 21 [====================================================================================================] 100.0% | Loss: 0.5321090221405029\n",
      "Epoch: 21 | Loss: 0.5321 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 22 [====================================================================================================] 100.0% | Loss: 0.5934113264083862\n",
      "Epoch: 22 | Loss: 0.5934 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 23 [====================================================================================================] 100.0% | Loss: 0.5572304725646973\n",
      "Epoch: 23 | Loss: 0.5572 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 24 [====================================================================================================] 100.0% | Loss: 0.5665186643600464\n",
      "Epoch: 24 | Loss: 0.5665 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 25 [====================================================================================================] 100.0% | Loss: 0.5636919736862183\n",
      "Epoch: 25 | Loss: 0.5637 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 26 [====================================================================================================] 100.0% | Loss: 0.5653133392333984\n",
      "Epoch: 26 | Loss: 0.5653 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 27 [====================================================================================================] 100.0% | Loss: 0.5652374029159546\n",
      "Epoch: 27 | Loss: 0.5652 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 28 [====================================================================================================] 100.0% | Loss: 0.5632959604263306\n",
      "Epoch: 28 | Loss: 0.5633 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 29 [====================================================================================================] 100.0% | Loss: 0.5606427192687988\n",
      "Epoch: 29 | Loss: 0.5606 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 30 [====================================================================================================] 100.0% | Loss: 0.5590285062789917\n",
      "Epoch: 30 | Loss: 0.5590 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 31 [====================================================================================================] 100.0% | Loss: 0.5790706276893616\n",
      "Epoch: 31 | Loss: 0.5791 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 32 [====================================================================================================] 100.0% | Loss: 0.5707452297210693\n",
      "Epoch: 32 | Loss: 0.5707 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 33 [====================================================================================================] 100.0% | Loss: 0.5597783327102661\n",
      "Epoch: 33 | Loss: 0.5598 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 34 [====================================================================================================] 100.0% | Loss: 0.5648419857025146\n",
      "Epoch: 34 | Loss: 0.5648 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 35 [====================================================================================================] 100.0% | Loss: 0.565880298614502\n",
      "Epoch: 35 | Loss: 0.5659 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 36 [====================================================================================================] 100.0% | Loss: 0.569331705570221\n",
      "Epoch: 36 | Loss: 0.5693 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 37 [====================================================================================================] 100.0% | Loss: 0.5621248483657837\n",
      "Epoch: 37 | Loss: 0.5621 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 38 [====================================================================================================] 100.0% | Loss: 0.5575246214866638\n",
      "Epoch: 38 | Loss: 0.5575 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 39 [====================================================================================================] 100.0% | Loss: 0.5719608068466187\n",
      "Epoch: 39 | Loss: 0.5720 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 40 [====================================================================================================] 100.0% | Loss: 0.5636986494064331\n",
      "Epoch: 40 | Loss: 0.5637 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 41 [====================================================================================================] 100.0% | Loss: 0.5631269812583923\n",
      "Epoch: 41 | Loss: 0.5631 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 42 [====================================================================================================] 100.0% | Loss: 0.560827374458313\n",
      "Epoch: 42 | Loss: 0.5608 | Val Accuracy: 37.25%\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 [====================================================================================================] 100.0% | Loss: 0.5688832402229309\n",
      "Epoch: 43 | Loss: 0.5689 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 44 [====================================================================================================] 100.0% | Loss: 0.5685583353042603\n",
      "Epoch: 44 | Loss: 0.5686 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 45 [====================================================================================================] 100.0% | Loss: 0.5696266889572144\n",
      "Epoch: 45 | Loss: 0.5696 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 46 [====================================================================================================] 100.0% | Loss: 0.5694922208786011\n",
      "Epoch: 46 | Loss: 0.5695 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 47 [====================================================================================================] 100.0% | Loss: 0.5614756941795349\n",
      "Epoch: 47 | Loss: 0.5615 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 48 [====================================================================================================] 100.0% | Loss: 0.5626303553581238\n",
      "Epoch: 48 | Loss: 0.5626 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 49 [====================================================================================================] 100.0% | Loss: 0.5672343969345093\n",
      "Epoch: 49 | Loss: 0.5672 | Val Accuracy: 37.25%\n",
      "\n",
      "Epoch: 50 [====================================================================================================] 100.0% | Loss: 0.5635876655578613\n",
      "Epoch: 50 | Loss: 0.5636 | Val Accuracy: 37.25%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(nn.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "loss_function = torch.nn.BCELoss()\n",
    "\n",
    "loss_arr = []\n",
    "acc_arr = []\n",
    "min_loss = None\n",
    "min_acc = 0\n",
    "outputs = []\n",
    "early_stop = False\n",
    "early_stop_depth = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "#     if epoch != 0:\n",
    "#         break\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    if early_stop:\n",
    "        if verbose != 0:\n",
    "            print(f'\\n\\n------EARLY STOP {min_loss}------\\n\\n')\n",
    "        break\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    nn.train()\n",
    "    nn.to(device)\n",
    "    for (image, label) in loader:\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        #encoded = encoder(image) # TO DO\n",
    "        output = nn(image)\n",
    "    \n",
    "#         print(output)\n",
    "#         print(label)\n",
    "        loss = loss_function(output, goal.unsqueeze(1).float())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "        # UI\n",
    "        if verbose == 2:\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(\"Epoch: {} [{:{}}] {:.1f}% | Loss: {}\".format(epoch+1, \"=\"*count, \n",
    "                                                                       len(loader)-1, \n",
    "                                                                       (100/(len(loader)-1)*count), \n",
    "                                                                       loss.item()))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        count += 1\n",
    "        \n",
    "#         break\n",
    "#     break\n",
    "    loss_arr.append(loss.item())\n",
    "    \n",
    "    nn.eval()\n",
    "    total_correct= 0\n",
    "    for (image_val, label_val) in val_loader:\n",
    "        nn.eval()\n",
    "        image_val = image_val.to(device)\n",
    "        label_val = label_val.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "#             _, val_outputs = torch.max(nn(image_val), 1)\n",
    "            val_outputs = torch.round(nn(image_val))\n",
    "            total_samples += 1\n",
    "            total_correct += (val_outputs == label_val).item()\n",
    "        \n",
    "    \n",
    "#     if early_stop_depth >= 1 and early_stop_depth < len(acc_arr[acc_arr.index(min_acc):]):\n",
    "#         early_stop = True\n",
    "#         for acc_item in acc_arr[acc_arr.index(min_loss):]:\n",
    "#             if acc_item < min_acc:\n",
    "#                 min_acc = acc_item\n",
    "#                 early_stop = False\n",
    "    \n",
    "        \n",
    "#     if not min_loss:\n",
    "#         min_loss = loss_arr[0]\n",
    "#     if early_stop_depth >= 1 and early_stop_depth < len(loss_arr[loss_arr.index(min_loss):]):\n",
    "#         early_stop = True\n",
    "#         for loss_item in loss_arr[loss_arr.index(min_loss):]:\n",
    "#             if loss_item < min_loss:\n",
    "#                 min_loss = loss_item\n",
    "#                 early_stop = False\n",
    "\n",
    "    accuracy = total_correct/len(val_loader)*100\n",
    "    if verbose != 0:\n",
    "        print(f'\\nEpoch: {epoch + 1} | Loss: {loss.item():.4f} | Val Accuracy: {accuracy:.2f}%', end='\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b089a5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 37.25%\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device('cpu')\n",
    "ans = []\n",
    "pre_sig = []\n",
    "total_samples = 0\n",
    "total_correct = 0\n",
    "nn.to(device)\n",
    "nn.eval()\n",
    "for (image, label) in val_loader:\n",
    "    nn.eval()\n",
    "    image = image.to(device)\n",
    "    label = label.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoded = nn.encoder(image)\n",
    "        not_rounded = nn._feed_forward(encoded.flatten(start_dim = 1))\n",
    "        pre_sig.append(not_rounded)\n",
    "        \n",
    "        #outputs = torch.round(nn._sigmoid(not_rounded))\n",
    "        outputs = nn._sigmoid(not_rounded)\n",
    "        if outputs < 0.5: # 0.779\n",
    "            outputs = 0\n",
    "        else:\n",
    "            outputs = 1\n",
    "        #_, outputs = torch.max(nn(image), 1) # learn more about the max function i guess?\n",
    "#         print(outputs)\n",
    "#         print(label)\n",
    "        total_samples += 1\n",
    "        total_correct += (outputs == label).item()\n",
    "        #ans.append((label.item(), outputs.item()))\n",
    "        ans.append((label.item(), outputs))\n",
    "\n",
    "        \n",
    "print(f'Accuracy: {total_correct/len(val_loader)*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0787ee86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48cd4a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = []\n",
    "with torch.no_grad():\n",
    "    nn.eval()\n",
    "#     print(val)\n",
    "#     m = torch.round(val)\n",
    "#     print(m.item())\n",
    "    for item in pre_sig:\n",
    "        val = nn._sigmoid(item)\n",
    "        sig.append(val.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac62058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "152c729e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x7feba85985e0>,\n",
       "  <matplotlib.lines.Line2D at 0x7feba85988b0>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x7feba8598b80>,\n",
       "  <matplotlib.lines.Line2D at 0x7feba8598e50>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x7feba8598310>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x7feba85a6160>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x7feba85a6430>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4759eb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7488048076629639\n",
      "0.7501294612884521\n",
      "0.7516307830810547\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.quantile(sig,0.25))\n",
    "print(np.quantile(sig,0.5))\n",
    "print(np.quantile(sig,0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dcfdcade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.747343897819519,\n",
       " 0.7507019639015198,\n",
       " 0.7499480247497559,\n",
       " 0.7494971752166748,\n",
       " 0.7503032684326172,\n",
       " 0.7440437078475952,\n",
       " 0.7498912811279297,\n",
       " 0.7497496008872986,\n",
       " 0.7512603402137756,\n",
       " 0.7491387128829956,\n",
       " 0.7537094950675964,\n",
       " 0.7498601675033569,\n",
       " 0.7528034448623657,\n",
       " 0.7518537044525146,\n",
       " 0.7512770891189575,\n",
       " 0.7528281211853027,\n",
       " 0.7466434836387634,\n",
       " 0.7517217993736267,\n",
       " 0.7504539489746094,\n",
       " 0.7494962811470032,\n",
       " 0.7521090507507324,\n",
       " 0.7525534629821777,\n",
       " 0.7548935413360596,\n",
       " 0.7492575645446777,\n",
       " 0.7501294612884521,\n",
       " 0.7494280338287354,\n",
       " 0.752791702747345,\n",
       " 0.7486748099327087,\n",
       " 0.748934805393219,\n",
       " 0.7517522573471069,\n",
       " 0.7511743903160095,\n",
       " 0.7460236549377441,\n",
       " 0.7504065036773682,\n",
       " 0.7539978623390198,\n",
       " 0.7511477470397949,\n",
       " 0.7463887929916382,\n",
       " 0.7494148015975952,\n",
       " 0.7504492402076721,\n",
       " 0.7467246055603027,\n",
       " 0.7480260729789734,\n",
       " 0.7515397667884827,\n",
       " 0.7483183741569519,\n",
       " 0.7502774000167847,\n",
       " 0.7498040795326233,\n",
       " 0.7540739178657532,\n",
       " 0.7480764389038086,\n",
       " 0.7483838796615601,\n",
       " 0.7480747699737549,\n",
       " 0.7447865605354309,\n",
       " 0.7511311769485474,\n",
       " 0.7522361278533936]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d122816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
