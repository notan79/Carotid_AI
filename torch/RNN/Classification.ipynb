{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a013e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from AutoEncoderCNN import AE_CNN\n",
    "from GridSearch import GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25a2eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb97273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/groups/francescavitali/eb2/subImages_slide299/H&E' # has 506 images\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "tensor_transform = transforms.ToTensor()\n",
    "\n",
    "dataset = datasets.ImageFolder(PATH, \n",
    "                               transform = tensor_transform) #loads the images\n",
    "\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset,\n",
    "                                                           [404,51,51],# 80%, 10%, 10%\n",
    "                                                           generator=torch.Generator(device=device))\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset = train_set,\n",
    "                                            batch_size = BATCH_SIZE,\n",
    "                                            shuffle = True,\n",
    "                                            generator=torch.Generator(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7ecbc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16cf6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AE_CNN(64,128).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('./models/model_gs.pth')) # loading best model state\n",
    "model.load_state_dict(torch.load('./models/Copy Models/model_gs_3-28-2024.pth'))\n",
    "\n",
    "# setting the encoder and decoder for visualization\n",
    "encoder = model.encoder\n",
    "decoder = model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de5fe0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Img Shape: torch.Size([4, 3, 299, 299])\n",
      "Encoded Shape: torch.Size([4, 128, 21, 21])\n",
      "Flattened: torch.Size([4, 56448])\n",
      "Goal: tensor([[0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0]], device='cuda:0')\n",
      "\n",
      "Img Shape: torch.Size([4, 3, 299, 299])\n",
      "Encoded Shape: torch.Size([4, 128, 21, 21])\n",
      "Flattened: torch.Size([4, 56448])\n",
      "Goal: tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0]], device='cuda:0')\n",
      "\n",
      "Img Shape: torch.Size([4, 3, 299, 299])\n",
      "Encoded Shape: torch.Size([4, 128, 21, 21])\n",
      "Flattened: torch.Size([4, 56448])\n",
      "Goal: tensor([[1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0]], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "with torch.no_grad():\n",
    "    for(img, goal) in loader: # goal will be a tensor of len == batch_size\n",
    "        if count == 3:\n",
    "            break\n",
    "        img = img.to(device)\n",
    "        print(f'Img Shape: {img.shape}')\n",
    "        encoded_img = encoder(img)\n",
    "        print(f'Encoded Shape: {encoded_img.shape}')\n",
    "        flattened = encoded_img.flatten(start_dim = 1)\n",
    "        print(f'Flattened: {flattened.shape}')\n",
    "        print(f'Goal: {goal.unsqueeze(1)}')\n",
    "        print()\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76a36303",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self._feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(56448, 2048),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(2048, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encoded_without_training(self, x):\n",
    "        with torch.no_grad():\n",
    "            encoded = self.encoder(x)\n",
    "            flattened = encoded.flatten(start_dim = 1)\n",
    "            return flattened\n",
    "    \n",
    "    def forward(self,  x):\n",
    "        #encoded = self.encoder(x) # this will update the encoder weights\n",
    "        #flattened = encoded.flatten(start_dim = 1)\n",
    "        flattened = self.encoded_without_training(x)\n",
    "        output = self._feed_forward(flattened)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20ca5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNet(encoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "849985bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00001\n",
    "weight_decay = 1e-5\n",
    "EPOCHS = 1000\n",
    "\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fb6d795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 | Loss: 0.6693\n",
      "\n",
      "\n",
      "Epoch: 2 | Loss: 0.6533\n",
      "\n",
      "\n",
      "Epoch: 3 | Loss: 0.7175\n",
      "\n",
      "\n",
      "Epoch: 4 | Loss: 0.6963\n",
      "\n",
      "\n",
      "Epoch: 5 | Loss: 0.6203\n",
      "\n",
      "\n",
      "Epoch: 6 | Loss: 0.6887\n",
      "\n",
      "\n",
      "Epoch: 7 | Loss: 0.6716\n",
      "\n",
      "\n",
      "Epoch: 8 | Loss: 0.7106\n",
      "\n",
      "\n",
      "Epoch: 9 | Loss: 0.7856\n",
      "\n",
      "\n",
      "Epoch: 10 | Loss: 0.7076\n",
      "\n",
      "\n",
      "Epoch: 11 | Loss: 0.7316\n",
      "\n",
      "\n",
      "Epoch: 12 | Loss: 0.7318\n",
      "\n",
      "\n",
      "Epoch: 13 | Loss: 0.7334\n",
      "\n",
      "\n",
      "Epoch: 14 | Loss: 0.7113\n",
      "\n",
      "\n",
      "Epoch: 15 | Loss: 0.6679\n",
      "\n",
      "\n",
      "Epoch: 16 | Loss: 0.7722\n",
      "\n",
      "\n",
      "Epoch: 17 | Loss: 0.6411\n",
      "\n",
      "\n",
      "Epoch: 18 | Loss: 0.7070\n",
      "\n",
      "\n",
      "Epoch: 19 | Loss: 0.6867\n",
      "\n",
      "\n",
      "Epoch: 20 | Loss: 0.6903\n",
      "\n",
      "\n",
      "Epoch: 21 | Loss: 0.6777\n",
      "\n",
      "\n",
      "Epoch: 22 | Loss: 0.7346\n",
      "\n",
      "\n",
      "Epoch: 23 | Loss: 0.6774\n",
      "\n",
      "\n",
      "Epoch: 24 | Loss: 0.6641\n",
      "\n",
      "\n",
      "Epoch: 25 | Loss: 0.7162\n",
      "\n",
      "\n",
      "\n",
      "------EARLY STOP 0.6203462481498718------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(nn.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "loss_function = torch.nn.BCELoss()\n",
    "\n",
    "loss_arr = []\n",
    "min_loss = None\n",
    "outputs = []\n",
    "early_stop = False\n",
    "early_stop_depth = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    if early_stop:\n",
    "        if verbose != 0:\n",
    "            print(f'\\n\\n------EARLY STOP {min_loss}------\\n\\n')\n",
    "        break\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    nn.train()\n",
    "    for (image, label) in loader:\n",
    "        image = image.to(device)\n",
    "        #image = image.flatten(start_dim=1) # ignore the batch_size\n",
    "\n",
    "        output = nn(image)\n",
    "        loss = loss_function(output, goal.unsqueeze(1).float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # UI\n",
    "        if verbose == 2:\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(\"Epoch: {} [{:{}}] {:.1f}% | Loss: {}\".format(epoch+1, \"=\"*count, \n",
    "                                                                       len(loader)-1, \n",
    "                                                                       (100/(len(loader)-1)*count), \n",
    "                                                                       loss.item()))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    loss_arr.append(loss.item())\n",
    "    if not min_loss:\n",
    "        min_loss = loss_arr[0]\n",
    "    if early_stop_depth >= 1 and early_stop_depth < len(loss_arr[loss_arr.index(min_loss):]):\n",
    "        early_stop = True\n",
    "        for loss_item in loss_arr[loss_arr.index(min_loss):]:\n",
    "            if loss_item < min_loss:\n",
    "                min_loss = loss_item\n",
    "                early_stop = False\n",
    "\n",
    "\n",
    "    if verbose != 0:\n",
    "        print(f'\\nEpoch: {epoch + 1} | Loss: {loss.item():.4f}', end='\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0261f598",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = []\n",
    "nn.eval()\n",
    "for x in range(len(val_set)):\n",
    "    with torch.no_grad():\n",
    "        inp = val_set.__getitem__(x)[0]\n",
    "        exp = val_set.__getitem__(x)[1]\n",
    "        pred = nn.cpu()(img).flatten() # why output as a tensor of shape 2 x 4?\n",
    "        ans.append((exp, float(min(pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be811f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
