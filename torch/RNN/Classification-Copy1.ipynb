{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a013e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from AutoEncoderCNN import AE_CNN\n",
    "from GridSearch import GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25a2eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a89fe42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb97273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/groups/francescavitali/eb2/subImages_slide299/H&E' # has 506 images\n",
    "BATCH_SIZE = 4 # make sure divisble by 404: 1, 2, 4, 101, 202, 404\n",
    "\n",
    "tensor_transform = transforms.ToTensor()\n",
    "\n",
    "dataset = datasets.ImageFolder(PATH, \n",
    "                               transform = tensor_transform) #loads the images\n",
    "\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset,\n",
    "                                                           [404,51,51],# 80%, 10%, 10%\n",
    "                                                           generator=torch.Generator(device=device))\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset = train_set,\n",
    "                                            batch_size = BATCH_SIZE,\n",
    "                                            shuffle = True,\n",
    "                                            generator=torch.Generator(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7ecbc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16cf6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AE_CNN(64,128).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('./models/model_gs.pth')) # loading best model state\n",
    "model.load_state_dict(torch.load('./models/Copy Models/model_gs_3-28-2024.pth'))\n",
    "\n",
    "# setting the encoder and decoder for visualization\n",
    "encoder = model.encoder\n",
    "decoder = model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de5fe0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Img Shape: torch.Size([4, 3, 299, 299])\n",
      "Encoded Shape: torch.Size([4, 128, 21, 21])\n",
      "Flattened: torch.Size([4, 56448])\n",
      "Goal: tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "with torch.no_grad():\n",
    "    for(img, goal) in loader: # goal will be a tensor of len == batch_size\n",
    "        if count == 1:\n",
    "            break\n",
    "        img = img.to(device)\n",
    "        print(f'Img Shape: {img.shape}')\n",
    "        encoded_img = encoder(img)\n",
    "        print(f'Encoded Shape: {encoded_img.shape}')\n",
    "        flattened = encoded_img.flatten(start_dim = 1)\n",
    "        print(f'Flattened: {flattened.shape}')\n",
    "        print(f'Goal: {goal.unsqueeze(1)}')\n",
    "        print()\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76a36303",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self._feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(56448, 32768),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32768, 16384),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16384, 8192),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(8192, 2048),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(2048, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 1),\n",
    "        )\n",
    "        self._sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def encoded_without_training(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.encoder.eval()\n",
    "            encoded = self.encoder(x)\n",
    "            flattened = encoded.flatten(start_dim = 1)\n",
    "            return flattened\n",
    "    \n",
    "    def forward(self,  x):\n",
    "        self.train()\n",
    "        encoded = self.encoder(x) # this will update the encoder weights\n",
    "        flattened = encoded.flatten(start_dim = 1)\n",
    "#         flattened = self.encoded_without_training(x)\n",
    "        output = self._feed_forward(flattened)\n",
    "        return self._sigmoid(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20ca5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNet(encoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "849985bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0003\n",
    "weight_decay = 1e-5\n",
    "EPOCHS = 15\n",
    "\n",
    "verbose = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06eaa0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(dataset = val_set,\n",
    "                                            batch_size = 1,\n",
    "                                            shuffle = True,\n",
    "                                            generator=torch.Generator(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb6d795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [===============================================                                                     ] 47.0% | Loss: 25.0489070892334"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(nn.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "loss_function = torch.nn.BCELoss()\n",
    "\n",
    "loss_arr = []\n",
    "acc_arr = []\n",
    "min_loss = None\n",
    "min_acc = 0\n",
    "outputs = []\n",
    "early_stop = False\n",
    "early_stop_depth = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "#     if epoch != 0:\n",
    "#         break\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    if early_stop:\n",
    "        if verbose != 0:\n",
    "            print(f'\\n\\n------EARLY STOP {min_loss}------\\n\\n')\n",
    "        break\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    nn.train()\n",
    "    nn.to(device)\n",
    "    for (image, label) in loader:\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        output = nn(image)\n",
    "    \n",
    "#         print(output)\n",
    "#         print(label)\n",
    "        loss = loss_function(output, goal.unsqueeze(1).float())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "        # UI\n",
    "        if verbose == 2:\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(\"Epoch: {} [{:{}}] {:.1f}% | Loss: {}\".format(epoch+1, \"=\"*count, \n",
    "                                                                       len(loader)-1, \n",
    "                                                                       (100/(len(loader)-1)*count), \n",
    "                                                                       loss.item()))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        count += 1\n",
    "        \n",
    "#         break\n",
    "#     break\n",
    "    loss_arr.append(loss.item())\n",
    "    \n",
    "    nn.eval()\n",
    "    total_correct= 0\n",
    "    for (image_val, label_val) in val_loader:\n",
    "        nn.eval()\n",
    "        image_val = image_val.to(device)\n",
    "        label_val = label_val.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "#             _, val_outputs = torch.max(nn(image_val), 1)\n",
    "            val_outputs = torch.round(nn(image_val))\n",
    "            total_samples += 1\n",
    "            total_correct += (val_outputs == label_val).item()\n",
    "        \n",
    "    \n",
    "#     if early_stop_depth >= 1 and early_stop_depth < len(acc_arr[acc_arr.index(min_acc):]):\n",
    "#         early_stop = True\n",
    "#         for acc_item in acc_arr[acc_arr.index(min_loss):]:\n",
    "#             if acc_item < min_acc:\n",
    "#                 min_acc = acc_item\n",
    "#                 early_stop = False\n",
    "    \n",
    "        \n",
    "#     if not min_loss:\n",
    "#         min_loss = loss_arr[0]\n",
    "#     if early_stop_depth >= 1 and early_stop_depth < len(loss_arr[loss_arr.index(min_loss):]):\n",
    "#         early_stop = True\n",
    "#         for loss_item in loss_arr[loss_arr.index(min_loss):]:\n",
    "#             if loss_item < min_loss:\n",
    "#                 min_loss = loss_item\n",
    "#                 early_stop = False\n",
    "\n",
    "    accuracy = total_correct/len(val_loader)*100\n",
    "    if verbose != 0:\n",
    "        print(f'\\nEpoch: {epoch + 1} | Loss: {loss.item():.4f} | Val Accuracy: {accuracy:.2f}%', end='\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b089a5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cpu')\n",
    "ans = []\n",
    "pre_sig = []\n",
    "total_samples = 0\n",
    "total_correct = 0\n",
    "nn.to(device)\n",
    "nn.eval()\n",
    "for (image, label) in val_loader:\n",
    "    nn.eval()\n",
    "    image = image.to(device)\n",
    "    label = label.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoded = nn.encoder(image)\n",
    "        not_rounded = nn._feed_forward(encoded.flatten(start_dim = 1))\n",
    "        pre_sig.append(not_rounded)\n",
    "        \n",
    "        #outputs = torch.round(nn._sigmoid(not_rounded))\n",
    "        outputs = nn._sigmoid(not_rounded)\n",
    "        if outputs < 0.5: # 0.779\n",
    "            outputs = 0\n",
    "        else:\n",
    "            outputs = 1\n",
    "        #_, outputs = torch.max(nn(image), 1) # learn more about the max function i guess?\n",
    "#         print(outputs)\n",
    "#         print(label)\n",
    "        total_samples += 1\n",
    "        total_correct += (outputs == label).item()\n",
    "        #ans.append((label.item(), outputs.item()))\n",
    "        ans.append((label.item(), outputs))\n",
    "\n",
    "        \n",
    "print(f'Accuracy: {total_correct/len(val_loader)*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0787ee86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (0, 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48cd4a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = []\n",
    "with torch.no_grad():\n",
    "    nn.eval()\n",
    "#     print(val)\n",
    "#     m = torch.round(val)\n",
    "#     print(m.item())\n",
    "    for item in pre_sig:\n",
    "        val = nn._sigmoid(item)\n",
    "        sig.append(val.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac62058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "152c729e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x7f330c749970>,\n",
       "  <matplotlib.lines.Line2D at 0x7f330c749c40>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x7f330c749fd0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f330c6ed220>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x7f330c7496a0>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x7f330c6ed4f0>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x7f330c6ed7c0>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4759eb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.733510822057724\n",
      "0.7383217811584473\n",
      "0.7450057566165924\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.quantile(sig,0.25))\n",
    "print(np.quantile(sig,0.5))\n",
    "print(np.quantile(sig,0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcfdcade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7273550033569336,\n",
       " 0.7473698854446411,\n",
       " 0.7245959043502808,\n",
       " 0.7289170622825623,\n",
       " 0.7342904806137085,\n",
       " 0.7296799421310425,\n",
       " 0.7435164451599121,\n",
       " 0.7328726053237915,\n",
       " 0.7456302642822266,\n",
       " 0.7456161975860596,\n",
       " 0.7441454529762268,\n",
       " 0.7355413436889648,\n",
       " 0.749972403049469,\n",
       " 0.735538125038147,\n",
       " 0.7341027855873108,\n",
       " 0.7425096035003662,\n",
       " 0.7388247847557068,\n",
       " 0.7424895763397217,\n",
       " 0.7421096563339233,\n",
       " 0.7392292618751526,\n",
       " 0.7275468111038208,\n",
       " 0.7435672879219055,\n",
       " 0.7330038547515869,\n",
       " 0.7367019057273865,\n",
       " 0.7417812943458557,\n",
       " 0.7475956082344055,\n",
       " 0.7368027567863464,\n",
       " 0.7327328324317932,\n",
       " 0.7341194152832031,\n",
       " 0.7538392543792725,\n",
       " 0.7456120252609253,\n",
       " 0.7527885437011719,\n",
       " 0.7379372715950012,\n",
       " 0.748124361038208,\n",
       " 0.7379562258720398,\n",
       " 0.7380146384239197,\n",
       " 0.7359899282455444,\n",
       " 0.7490084171295166,\n",
       " 0.7340177893638611,\n",
       " 0.7274972796440125,\n",
       " 0.743715226650238,\n",
       " 0.7311476469039917,\n",
       " 0.7476195096969604,\n",
       " 0.7146974205970764,\n",
       " 0.7399376630783081,\n",
       " 0.7443994879722595,\n",
       " 0.7383217811584473,\n",
       " 0.7520117163658142,\n",
       " 0.7226454019546509,\n",
       " 0.7101054787635803,\n",
       " 0.7480121850967407]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d122816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
